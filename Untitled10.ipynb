{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSMudly/CHAT-APP/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of7dBtB0TfPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec5e08a-c520-4e3c-e0eb-96b1cf39b7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   post_id                                               post      label\n",
            "0        1  I don't know how much more I can take. Everyth...  depressed\n",
            "1        2  Presentation tomorrow and I haven’t even start...   stressed\n",
            "2        3   Finally had a peaceful day. No drama, just calm.     normal\n",
            "3        4  It’s like I’m screaming inside but nobody can ...  depressed\n",
            "4        5  Too many deadlines, too little time. Why is li...   stressed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your dataset path in Drive\n",
        "file_path = \"/content/drive/MyDrive/SDN_text_database (1).xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display first 5 rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall numpy==1.26.0 gensim==4.3.3 protobuf==3.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "XgCmIwXc8eLX",
        "outputId": "7cb11204-569f-47bc-cae9-0dfdc9430b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.0\n",
            "  Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting gensim==4.3.3\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting protobuf==3.20.3\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim==4.3.3)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim==4.3.3)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim==4.3.3)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, protobuf, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.0\n",
            "    Uninstalling numpy-1.26.0:\n",
            "      Successfully uninstalled numpy-1.26.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.0 protobuf-3.20.3 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "8c6571f2a4b641809cab22fb3c14b02b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Yam-0Vb2dM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd603299-a708-4deb-9af4-9deee2627508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Using cached contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Using cached textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Using cached anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Using cached pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "Using cached pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import contractions\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Initialize stopwords and punctuation\n",
        "stopwords = set(stopwords.words('english'))  # Using set for faster lookups\n",
        "exclude = string.punctuation\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    text = contractions.fix(text)  # Fix contractions first\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "        if word.lower() not in stopwords:  # Case-insensitive check\n",
        "            new_text.append(word)\n",
        "    return ' '.join(new_text)\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('', '', exclude))  # Corrected punctuation removal\n",
        "\n",
        "# Tokenize text\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Get POS tag for lemmatization\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize text\n",
        "def lemmatize_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
        "    return lemmatized\n",
        "\n",
        "# Convert text to word vectors\n",
        "def sequence(text, model):\n",
        "    vec = [model[word] if word in model else np.zeros(model.vector_size) for word in text]\n",
        "    return vec\n",
        "\n",
        "# Pad vector sequences to fixed length\n",
        "def pad_vector_sequence(seq, max_length, vector_size):\n",
        "    padded = np.zeros((max_length, vector_size))\n",
        "    for i in range(min(len(seq), max_length)):\n",
        "        padded[i] = seq[i]\n",
        "    return padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NdCAo9GfvvS"
      },
      "outputs": [],
      "source": [
        "df[\"cleaned_text\"] = df[\"post\"].apply(remove_stopwords)\n",
        "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(remove_punc)\n",
        "df[\"lemmatized_text\"] = df[\"cleaned_text\"].apply(lemmatize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNgS3pcDlp3k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "9f9092bb-e068-4233-93da-cc6c001805c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max text length: 640\n",
            "Mean text length: 53.55655655655656\n",
            "Median text length: 39.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95th percentile length: 143\n",
            "\n",
            "Sample original text:\n",
            "I don't know how much more I can take. Everything feels so heavy.\n",
            "\n",
            "Processed text:\n",
            "know much take everything feel heavy\n",
            "\n",
            "Lemmatized tokens:\n",
            "['know', 'much', 'take', 'everything', 'feel', 'heavy']\n"
          ]
        }
      ],
      "source": [
        "# Convert lemmatized tokens back to string for GloVe processing (if needed)\n",
        "df['processed_text'] = df['lemmatized_text'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Optional: Check the distribution of text lengths to determine padding length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_lengths = df['lemmatized_text'].apply(len)\n",
        "print(f\"Max text length: {text_lengths.max()}\")\n",
        "print(f\"Mean text length: {text_lengths.mean()}\")\n",
        "print(f\"Median text length: {text_lengths.median()}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(text_lengths, bins=50)\n",
        "plt.title('Distribution of Text Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Set max_length based on your analysis (e.g., 95th percentile)\n",
        "percentile_95 = int(np.percentile(text_lengths, 95))\n",
        "print(f\"95th percentile length: {percentile_95}\")\n",
        "max_length = percentile_95  # You can adjust this based on your needs\n",
        "\n",
        "# Or simply use the maximum length\n",
        "# max_length = text_lengths.max()\n",
        "\n",
        "# Verify the preprocessing results\n",
        "print(\"\\nSample original text:\")\n",
        "print(df['post'].iloc[0])\n",
        "print(\"\\nProcessed text:\")\n",
        "print(df['processed_text'].iloc[0])\n",
        "print(\"\\nLemmatized tokens:\")\n",
        "print(df['lemmatized_text'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "\n",
        "# Download and extract only the 100d GloVe embeddings\n",
        "glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "glove_zip_path = '/tmp/glove_temp.zip'\n",
        "glove_txt_path = '/tmp/glove.6B.100d.txt'\n",
        "\n",
        "# Download the zip file if not already exists\n",
        "if not os.path.exists(glove_txt_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    !wget {glove_url} -O {glove_zip_path}\n",
        "\n",
        "    # Extract only the 100d version\n",
        "    with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extract('glove.6B.100d.txt', path='/tmp/')\n",
        "\n",
        "    # Clean up the zip file\n",
        "    os.remove(glove_zip_path)\n",
        "    print(\"GloVe embeddings downloaded and extracted.\")\n",
        "\n",
        "# Load the embeddings directly into memory\n",
        "print(\"Loading GloVe model...\")\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_txt_path, binary=False, no_header=True)\n",
        "print(f\"GloVe model loaded with {len(glove_model)} word vectors.\")\n",
        "\n",
        "# Text processing functions\n",
        "def sequence(text, model):\n",
        "    \"\"\"Convert text to sequence of word vectors\"\"\"\n",
        "    vec = [model[word] if word in model else np.zeros(model.vector_size) for word in text]\n",
        "    return vec\n",
        "\n",
        "def pad_vector_sequence(seq, max_length, vector_size):\n",
        "    \"\"\"Pad sequence to fixed length\"\"\"\n",
        "    padded = np.zeros((max_length, vector_size))\n",
        "    for i in range(min(len(seq), max_length)):\n",
        "        padded[i] = seq[i]\n",
        "    return padded\n",
        "\n",
        "# Example usage with your DataFrame\n",
        "# Assuming 'data' is your DataFrame with 'post' column\n",
        "if 'data' in locals():\n",
        "    print(\"Creating embeddings for your text data...\")\n",
        "    data['glove'] = data['post'].apply(lambda text: sequence(text, glove_model))\n",
        "    glove_sentences = data['glove'].tolist()\n",
        "    max_length = data['post'].apply(len).max()\n",
        "    padded_glove = np.stack([pad_vector_sequence(seq, max_length, glove_model.vector_size) for seq in glove_sentences])\n",
        "\n",
        "    print(\"Embeddings created successfully!\")\n",
        "    print(f\"Final embeddings shape: {padded_glove.shape}\")\n",
        "else:\n",
        "    print(\"GloVe model ready to use. Call glove_model['word'] to get vectors.\")\n",
        "\n",
        "\n",
        "# Add these functions to your existing code\n",
        "def get_sentence_embedding(tokens):\n",
        "    \"\"\"Convert list of tokens to a single sentence embedding by averaging word vectors\"\"\"\n",
        "    valid_vectors = [glove_model[word] for word in tokens if word in glove_model]\n",
        "    if not valid_vectors:  # If no words in vocabulary\n",
        "        return np.zeros(glove_model.vector_size)\n",
        "    return np.mean(valid_vectors, axis=0)\n",
        "\n",
        "# Add this after your text preprocessing steps\n",
        "df[\"glove_embeddings\"] = df[\"lemmatized_text\"].apply(get_sentence_embedding)\n",
        "\n",
        "# Convert to numpy matrix (optional)\n",
        "embeddings_matrix = np.vstack(df[\"glove_embeddings\"].values)\n",
        "\n",
        "# After creating the embeddings, add this to print them\n",
        "print(\"\\nSample GloVe Embeddings:\")\n",
        "for i, (text, embedding) in enumerate(zip(df['lemmatized_text'], df['glove_embeddings'])):\n",
        "    print(f\"\\nSentence {i+1}: {text}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(f\"First 10 dimensions: {embedding[:10]}\")  # Print first 10 dimensions for readability\n",
        "\n",
        "    # Stop after printing 3 examples to avoid clutter\n",
        "    if i == 2:\n",
        "        break\n",
        "\n",
        "# Print the full embeddings matrix shape\n",
        "print(\"\\nFull Embeddings Matrix:\")\n",
        "print(f\"Shape: {embeddings_matrix.shape}\")\n",
        "print(f\"Sample row (first 10 dims): {embeddings_matrix[0][:10]}\")"
      ],
      "metadata": {
        "id": "2ESQiu_QaDCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e85548-d7e8-49fc-8be3-eeeb6ece6c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "--2025-06-26 20:08:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-06-26 20:08:36--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/tmp/glove_temp.zip’\n",
            "\n",
            "/tmp/glove_temp.zip 100%[===================>] 822.24M  3.46MB/s    in 6m 6s   \n",
            "\n",
            "2025-06-26 20:14:42 (2.25 MB/s) - ‘/tmp/glove_temp.zip’ saved [862182613/862182613]\n",
            "\n",
            "GloVe embeddings downloaded and extracted.\n",
            "Loading GloVe model...\n",
            "GloVe model loaded with 400000 word vectors.\n",
            "GloVe model ready to use. Call glove_model['word'] to get vectors.\n",
            "\n",
            "Sample GloVe Embeddings:\n",
            "\n",
            "Sentence 1: ['know', 'much', 'take', 'everything', 'feel', 'heavy']\n",
            "Embedding shape: (100,)\n",
            "First 10 dimensions: [-0.24506317  0.4380583   0.44744834 -0.25248066 -0.511455    0.10312137\n",
            " -0.39116502  0.157094    0.02870434 -0.05362184]\n",
            "\n",
            "Sentence 2: ['presentation', 'tomorrow', 'even', 'start', 'panic']\n",
            "Embedding shape: (100,)\n",
            "First 10 dimensions: [ 0.18647179  0.149852    0.291806   -0.12327732 -0.35342798  0.14193559\n",
            " -0.15371022  0.19972196  0.01584799 -0.24969402]\n",
            "\n",
            "Sentence 3: ['finally', 'peaceful', 'day', 'drama', 'calm']\n",
            "Embedding shape: (100,)\n",
            "First 10 dimensions: [ 0.02360981  0.01901399  0.122032    0.184202   -0.043569    0.44513923\n",
            " -0.057694    0.34961534 -0.17850402 -0.08064   ]\n",
            "\n",
            "Full Embeddings Matrix:\n",
            "Shape: (999, 100)\n",
            "Sample row (first 10 dims): [-0.24506317  0.4380583   0.44744834 -0.25248066 -0.511455    0.10312137\n",
            " -0.39116502  0.157094    0.02870434 -0.05362184]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you already have:\n",
        "# df['glove_embeddings'] - your precomputed GloVe embeddings\n",
        "# df['label'] - your target variable\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "X = np.vstack(df['glove_embeddings'].values)\n",
        "y = df['label'].values  # Replace 'label' with your actual target column name\n",
        "\n",
        "# Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,  # For reproducibility\n",
        "    stratify=y  # Preserves class distribution\n",
        ")\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"Training set shapes:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(\"\\nTest set shapes:\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# Optional: View first few samples\n",
        "print(\"\\nFirst training sample embedding (first 10 dims):\", X_train[0][:10])\n",
        "print(\"Corresponding label:\", y_train[0])"
      ],
      "metadata": {
        "id": "jhLfc2sRdrTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b6f35d-d164-44c0-8897-ab6bfe632639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shapes:\n",
            "X_train: (799, 100), y_train: (799,)\n",
            "\n",
            "Test set shapes:\n",
            "X_test: (200, 100), y_test: (200,)\n",
            "\n",
            "First training sample embedding (first 10 dims): [-0.02733208  0.40034065  0.36928502 -0.07914472 -0.29385537  0.04931592\n",
            " -0.20589872  0.01950649  0.28765202 -0.00550916]\n",
            "Corresponding label: depressed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Masking, Bidirectional, LSTM,\n",
        "                                   Dense, Dropout, LayerNormalization,\n",
        "                                   MultiHeadAttention, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Prepare your data\n",
        "X = np.vstack(df['glove_embeddings'].values)\n",
        "\n",
        "# Convert string labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'].values)  # Convert strings to integers\n",
        "\n",
        "# Reshape X to 3D for LSTM (samples, timesteps=1, features)\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# 2. Build the model\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "masked = Masking(mask_value=0.0)(inputs)\n",
        "\n",
        "# Bidirectional LSTM\n",
        "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(masked)\n",
        "\n",
        "# MultiHeadAttention\n",
        "attention_output = MultiHeadAttention(\n",
        "    num_heads=4,\n",
        "    key_dim=16\n",
        ")(lstm_out, lstm_out)\n",
        "\n",
        "# Pooling\n",
        "pooled = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "# Classification head\n",
        "norm1 = LayerNormalization()(pooled)\n",
        "drop1 = Dropout(0.4)(norm1)\n",
        "dense1 = Dense(64, activation='relu', kernel_regularizer=l2(0.02))(drop1)\n",
        "norm2 = LayerNormalization()(dense1)\n",
        "drop2 = Dropout(0.3)(norm2)\n",
        "outputs = Dense(len(label_encoder.classes_), activation='softmax')(drop2)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001, restore_best_weights=True),\n",
        "    ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "# 5. Train the model\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Evaluate\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# 7. (Optional) To convert predictions back to original labels\n",
        "# y_pred = model.predict(X_test)\n",
        "# y_pred_labels = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))"
      ],
      "metadata": {
        "id": "8cCgT-SJb4Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce44042-fed8-4865-aac5-14c4ad9ed93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.3565 - loss: 3.1045 - val_accuracy: 0.3850 - val_loss: 2.5500\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3569 - loss: 2.6467 - val_accuracy: 0.4800 - val_loss: 2.2800\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4385 - loss: 2.2655 - val_accuracy: 0.5300 - val_loss: 2.0417\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5162 - loss: 2.0091 - val_accuracy: 0.5350 - val_loss: 1.8293\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5876 - loss: 1.7408 - val_accuracy: 0.6050 - val_loss: 1.6228\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5805 - loss: 1.6234 - val_accuracy: 0.5950 - val_loss: 1.5894\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6402 - loss: 1.4225 - val_accuracy: 0.5550 - val_loss: 1.4853\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6041 - loss: 1.3535 - val_accuracy: 0.6350 - val_loss: 1.3429\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6460 - loss: 1.2292 - val_accuracy: 0.6350 - val_loss: 1.2474\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6608 - loss: 1.1197 - val_accuracy: 0.5700 - val_loss: 1.3436\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.6540 - loss: 1.1827 - val_accuracy: 0.6350 - val_loss: 1.1713\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.6652 - loss: 1.0598 - val_accuracy: 0.6050 - val_loss: 1.1633\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6750 - loss: 1.0006 - val_accuracy: 0.6350 - val_loss: 1.1297\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7308 - loss: 0.9083 - val_accuracy: 0.6500 - val_loss: 1.0803\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7330 - loss: 0.8460 - val_accuracy: 0.6550 - val_loss: 1.0895\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7360 - loss: 0.8222 - val_accuracy: 0.5900 - val_loss: 1.0712\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6839 - loss: 0.8956 - val_accuracy: 0.6500 - val_loss: 1.0325\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7481 - loss: 0.7337 - val_accuracy: 0.6300 - val_loss: 0.9835\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7254 - loss: 0.7476 - val_accuracy: 0.6550 - val_loss: 1.0203\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7371 - loss: 0.7386 - val_accuracy: 0.6550 - val_loss: 0.9720\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7568 - loss: 0.7141 - val_accuracy: 0.6500 - val_loss: 0.9895\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7646 - loss: 0.6532 - val_accuracy: 0.6750 - val_loss: 0.9094\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7589 - loss: 0.6300 - val_accuracy: 0.6350 - val_loss: 1.1225\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7419 - loss: 0.7183 - val_accuracy: 0.6150 - val_loss: 0.9139\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7511 - loss: 0.6637 - val_accuracy: 0.6750 - val_loss: 0.9704\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7268 - loss: 0.7289 - val_accuracy: 0.6650 - val_loss: 0.8626\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7800 - loss: 0.6167 - val_accuracy: 0.6700 - val_loss: 0.8887\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7112 - loss: 0.6884 - val_accuracy: 0.6850 - val_loss: 1.0112\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7562 - loss: 0.6401 - val_accuracy: 0.6600 - val_loss: 0.9136\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7727 - loss: 0.5706 - val_accuracy: 0.6550 - val_loss: 0.9725\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7897 - loss: 0.5758 - val_accuracy: 0.6600 - val_loss: 1.0111\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7844 - loss: 0.5833 - val_accuracy: 0.6200 - val_loss: 0.9698\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.7271 - loss: 0.6925 - val_accuracy: 0.6600 - val_loss: 0.8458\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7700 - loss: 0.6241 - val_accuracy: 0.6500 - val_loss: 0.8961\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7698 - loss: 0.5858 - val_accuracy: 0.6700 - val_loss: 0.8863\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7475 - loss: 0.6187 - val_accuracy: 0.6450 - val_loss: 0.8919\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8153 - loss: 0.5001 - val_accuracy: 0.6650 - val_loss: 0.9492\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7807 - loss: 0.5735 - val_accuracy: 0.6700 - val_loss: 1.0230\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8063 - loss: 0.5385 - val_accuracy: 0.6450 - val_loss: 1.0573\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7181 - loss: 0.6826 - val_accuracy: 0.6700 - val_loss: 0.8940\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7965 - loss: 0.5149 - val_accuracy: 0.6850 - val_loss: 0.9517\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8216 - loss: 0.4980 - val_accuracy: 0.6400 - val_loss: 1.1352\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7287 - loss: 0.7153 - val_accuracy: 0.6550 - val_loss: 0.8705\n",
            "\n",
            "Evaluating model...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7012 - loss: 0.7410\n",
            "\n",
            "Final Test Accuracy: 0.6600\n",
            "Final Test Loss: 0.8458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Masking, Bidirectional, LSTM,\n",
        "                                   Dense, Dropout, LayerNormalization,\n",
        "                                   MultiHeadAttention, GlobalAveragePooling1D,\n",
        "                                   BatchNormalization, Concatenate)\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
        "                                      ReduceLROnPlateau)\n",
        "\n",
        "# 1. Data Preparation\n",
        "X = np.vstack(df['glove_embeddings'].values)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'].values)\n",
        "\n",
        "# Class weights\n",
        "class_counts = np.bincount(y)\n",
        "class_weights = {i: 1./count for i, count in enumerate(class_counts)}\n",
        "class_weights = {k: v / sum(class_weights.values()) for k, v in class_weights.items()}\n",
        "\n",
        "# Reshape X to 3D\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Fixed Model Architecture for single-timestep sequences\n",
        "def build_enhanced_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    masked = Masking(mask_value=0.0)(inputs)\n",
        "\n",
        "    # BiLSTM Layer\n",
        "    lstm_out = Bidirectional(LSTM(128, return_sequences=True,\n",
        "                               kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))(masked)\n",
        "    lstm_out = BatchNormalization()(lstm_out)\n",
        "\n",
        "    # Reshape for attention (add sequence length dimension)\n",
        "    reshaped = tf.keras.layers.Reshape((1, -1))(lstm_out)\n",
        "\n",
        "    # Single Attention Layer (simplified for single-timestep)\n",
        "    attention_output = MultiHeadAttention(\n",
        "        num_heads=4,\n",
        "        key_dim=32,\n",
        "        dropout=0.3\n",
        "    )(reshaped, reshaped)\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    # Pooling and classification\n",
        "    pooled = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "    x = Dense(128, activation='relu',\n",
        "              kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(pooled)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(64, activation='relu',\n",
        "              kernel_regularizer=l1_l2(l1=0.005, l2=0.005))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# 3. Build model\n",
        "model = build_enhanced_model(\n",
        "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "    num_classes=len(label_encoder.classes_)\n",
        ")\n",
        "\n",
        "# 4. Training Configuration\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_accuracy'),\n",
        "    ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5)\n",
        "]\n",
        "\n",
        "# 5. Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# 6. Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "_i6n74UvelFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# If you used LabelEncoder, get the original class names\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# 2. Classification Report (includes F1 score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=class_names))\n",
        "\n",
        "# 3. F1 Scores (macro, micro, weighted)\n",
        "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
        "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
        "f1_weighted = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "print(\"\\nF1 Scores:\")\n",
        "print(f\"Macro F1: {f1_macro:.4f}\")\n",
        "print(f\"Micro F1: {f1_micro:.4f}\")\n",
        "print(f\"Weighted F1: {f1_weighted:.4f}\")\n",
        "\n",
        "# 4. Per-class F1 Scores\n",
        "per_class_f1 = f1_score(y_test, y_pred_classes, average=None)\n",
        "print(\"\\nPer-class F1 Scores:\")\n",
        "for i, f1 in enumerate(per_class_f1):\n",
        "    print(f\"{class_names[i]}: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "OoZH2sWzIHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Masking, Dense, Dropout,\n",
        "                                   LayerNormalization, MultiHeadAttention,\n",
        "                                   GlobalAveragePooling1D)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Prepare your data\n",
        "X = np.vstack(df['glove_embeddings'].values)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'].values)\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])  # (samples, timesteps=1, features)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2. Enhanced Transformer Model with 2 Attention Layers\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # First Attention Layer\n",
        "    attn_output1 = MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    attn_output1 = Dropout(dropout)(attn_output1)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output1)\n",
        "\n",
        "    # Second Attention Layer (new addition)\n",
        "    attn_output2 = MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(out1, out1)\n",
        "    attn_output2 = Dropout(dropout)(attn_output2)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn_output2)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(out2)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    return LayerNormalization(epsilon=1e-6)(out2 + ffn_output)\n",
        "\n",
        "def build_transformer_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Masking(mask_value=0.0)(inputs)\n",
        "\n",
        "    # Transformer Block with enhanced attention\n",
        "    x = transformer_encoder(x, head_size=32, num_heads=4, ff_dim=64, dropout=0.3)\n",
        "\n",
        "    # Pooling and Classification (same as before)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.02))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# 3. Build, compile and train (same as before)\n",
        "model = build_transformer_model(\n",
        "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "    num_classes=len(label_encoder.classes_)\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint(\"best_transformer_2attention.keras\", save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. Evaluation (same as before)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes,\n",
        "                           target_names=label_encoder.classes_))\n",
        "\n",
        "# F1 Scores\n",
        "print(f\"Macro F1: {f1_score(y_test, y_pred_classes, average='macro'):.4f}\")\n",
        "print(f\"Weighted F1: {f1_score(y_test, y_pred_classes, average='weighted'):.4f}\")"
      ],
      "metadata": {
        "id": "7Ck5Cb9js_3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9b8103e-6e82-4447-9789-919623cadf15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3601 - loss: 3.1092"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 59ms/step - accuracy: 0.3606 - loss: 3.0957 - val_accuracy: 0.3550 - val_loss: 2.4793\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3977 - loss: 2.5826 - val_accuracy: 0.4700 - val_loss: 2.3116\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4250 - loss: 2.3780 - val_accuracy: 0.4700 - val_loss: 2.1053\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.4072 - loss: 2.1890 - val_accuracy: 0.5200 - val_loss: 1.9553\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4637 - loss: 2.0295 - val_accuracy: 0.5950 - val_loss: 1.8242\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5035 - loss: 1.8717 - val_accuracy: 0.4800 - val_loss: 1.7836\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5080 - loss: 1.7764 - val_accuracy: 0.5300 - val_loss: 1.6727\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5234 - loss: 1.6726 - val_accuracy: 0.5550 - val_loss: 1.5762\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.5412 - loss: 1.5257 - val_accuracy: 0.6100 - val_loss: 1.4304\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.5632 - loss: 1.4333 - val_accuracy: 0.6250 - val_loss: 1.3402\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.6042 - loss: 1.3757 - val_accuracy: 0.6250 - val_loss: 1.2706\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5577 - loss: 1.3141 - val_accuracy: 0.6400 - val_loss: 1.2012\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5807 - loss: 1.2456 - val_accuracy: 0.6450 - val_loss: 1.1685\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6316 - loss: 1.1532 - val_accuracy: 0.6400 - val_loss: 1.1400\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6251 - loss: 1.1341 - val_accuracy: 0.6600 - val_loss: 1.0904\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6467 - loss: 1.1143 - val_accuracy: 0.6400 - val_loss: 1.0812\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6293 - loss: 1.0733 - val_accuracy: 0.6450 - val_loss: 1.0570\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6120 - loss: 1.0498 - val_accuracy: 0.6350 - val_loss: 1.0546\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6497 - loss: 0.9853 - val_accuracy: 0.6650 - val_loss: 1.0433\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6942 - loss: 0.9067 - val_accuracy: 0.6600 - val_loss: 1.0274\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6699 - loss: 0.9335 - val_accuracy: 0.6700 - val_loss: 0.9716\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6855 - loss: 0.9062 - val_accuracy: 0.6750 - val_loss: 0.9687\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6947 - loss: 0.8523 - val_accuracy: 0.6650 - val_loss: 0.9480\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6531 - loss: 0.8907 - val_accuracy: 0.5900 - val_loss: 1.0206\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6711 - loss: 0.8701 - val_accuracy: 0.6600 - val_loss: 0.9196\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.6873 - loss: 0.8441 - val_accuracy: 0.6750 - val_loss: 0.9112\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.7061 - loss: 0.7887 - val_accuracy: 0.6700 - val_loss: 0.9083\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6901 - loss: 0.7808 - val_accuracy: 0.6550 - val_loss: 0.9094\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.7162 - loss: 0.7994 - val_accuracy: 0.6350 - val_loss: 0.9657\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6522 - loss: 0.8355 - val_accuracy: 0.6400 - val_loss: 0.8811\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6627 - loss: 0.8328 - val_accuracy: 0.6600 - val_loss: 0.8725\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7030 - loss: 0.7619 - val_accuracy: 0.6500 - val_loss: 0.8784\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7360 - loss: 0.7451 - val_accuracy: 0.6250 - val_loss: 0.9311\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7133 - loss: 0.7325 - val_accuracy: 0.6250 - val_loss: 0.9720\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6886 - loss: 0.7555 - val_accuracy: 0.6800 - val_loss: 0.8498\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6993 - loss: 0.7282 - val_accuracy: 0.6400 - val_loss: 0.8700\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6845 - loss: 0.7699 - val_accuracy: 0.6850 - val_loss: 0.8077\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7001 - loss: 0.7478 - val_accuracy: 0.6800 - val_loss: 0.8076\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7056 - loss: 0.7374 - val_accuracy: 0.6900 - val_loss: 0.8713\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7348 - loss: 0.6581 - val_accuracy: 0.6850 - val_loss: 0.8329\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7332 - loss: 0.6692 - val_accuracy: 0.6700 - val_loss: 0.8077\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7603 - loss: 0.6390 - val_accuracy: 0.7100 - val_loss: 0.8305\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7033 - loss: 0.6975 - val_accuracy: 0.6800 - val_loss: 0.8606\n",
            "Epoch 44/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7643 - loss: 0.6724 - val_accuracy: 0.6800 - val_loss: 0.8859\n",
            "Epoch 45/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7593 - loss: 0.6472 - val_accuracy: 0.6750 - val_loss: 0.8154\n",
            "Epoch 46/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7532 - loss: 0.6662 - val_accuracy: 0.6750 - val_loss: 0.8283\n",
            "Epoch 47/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.7599 - loss: 0.6416 - val_accuracy: 0.6700 - val_loss: 0.8294\n",
            "Epoch 48/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.7634 - loss: 0.6398 - val_accuracy: 0.6750 - val_loss: 0.8562\n",
            "\n",
            "Test Accuracy: 0.6800\n",
            "Test Loss: 0.8076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7af9cbe011c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 548ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   depressed       0.70      0.85      0.77        66\n",
            "      normal       0.81      0.51      0.62        67\n",
            "    stressed       0.59      0.69      0.63        67\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.70      0.68      0.68       200\n",
            "weighted avg       0.70      0.68      0.67       200\n",
            "\n",
            "Macro F1: 0.6752\n",
            "Weighted F1: 0.6747\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj4eMkQzn2SvqjHOKzlxI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}